<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>DeepTech Daily Dashboard</title>
  <style>
body { font-family: 'Inter', 'Segoe UI', system-ui, -apple-system, sans-serif; margin: 0; padding: 0; background: #0f172a; color: #e2e8f0; }
header { padding: 2.5rem 1.5rem; text-align: center; background: #1e293b; }
header h1 { margin: 0 0 0.5rem; font-size: 2.5rem; }
header p { margin: 0; color: #94a3b8; font-size: 1rem; }
main { max-width: 1100px; margin: 0 auto; padding: 2rem 1.5rem 4rem; }
section { background: rgba(15, 23, 42, 0.75); border: 1px solid rgba(148, 163, 184, 0.2); border-radius: 12px; padding: 1.5rem; margin-bottom: 2rem; box-shadow: 0 12px 24px rgba(15, 23, 42, 0.4); }
section h2 { margin-top: 0; color: #f8fafc; font-size: 1.5rem; }
table { width: 100%; border-collapse: collapse; margin-top: 1rem; }
th, td { text-align: left; padding: 0.65rem 0.75rem; border-bottom: 1px solid rgba(148, 163, 184, 0.2); }
th { background: rgba(148, 163, 184, 0.1); color: #e2e8f0; font-weight: 600; }
tr:hover td { background: rgba(59, 130, 246, 0.08); }
a { color: #38bdf8; text-decoration: none; }
a:hover { text-decoration: underline; }
.warning { margin: 0.75rem 0; color: #fbbf24; }
.empty { margin: 1rem 0 0; color: #94a3b8; font-style: italic; }
.metadata { margin: 0.35rem 0 0; color: #94a3b8; font-size: 0.9rem; }
footer { text-align: center; padding: 2rem 1.5rem 3rem; color: #64748b; font-size: 0.9rem; }
footer ul { list-style: none; padding: 0; margin: 1rem 0 0; display: flex; flex-wrap: wrap; gap: 0.75rem; justify-content: center; }
footer li { background: rgba(148, 163, 184, 0.1); border-radius: 6px; padding: 0.4rem 0.75rem; }
</style>
</head>
<body>
  <header>
    <h1>DeepTech Daily Dashboard</h1>
    <p>A curated snapshot of GPUs, research, software, and security intel powered by the DeepTech Daily datasets.</p>
  </header>
  <main>
    <section>
  <h2>GPU Pricing Snapshot</h2>
  <p class="metadata">Fetched at 2025-10-17T00:47:37Z ‚Ä¢ Source: <a href="https://vast.ai/api/v0/bundles/public" target="_blank" rel="noopener">https://vast.ai/api/v0/bundles/public</a>, <a href="https://api.runpod.io" target="_blank" rel="noopener">https://api.runpod.io</a>, <a href="https://modal.com/api" target="_blank" rel="noopener">https://modal.com/api</a> ‚Ä¢ Hash: <code>73cd0d24819c38ed4bb8d48de66e8dd1013b8b60870640a96bc6c90d3b6d6974</code></p>
  <p class="warning">Vast.ai: 404 Client Error: Not Found for url: https://console.vast.ai/api/v0/bundles/public/?limit=200&amp;skip=0</p>
  <p class="empty">No data available.</p>
</section><section>
  <h2>arXiv Digest</h2>
  <p class="metadata">Fetched at 2025-11-13T05:33:08Z ‚Ä¢ Source: <a href="https://export.arxiv.org/rss/cs.AI" target="_blank" rel="noopener">https://export.arxiv.org/rss/cs.AI</a>, <a href="https://export.arxiv.org/rss/cs.CL" target="_blank" rel="noopener">https://export.arxiv.org/rss/cs.CL</a>, <a href="https://export.arxiv.org/rss/cs.LG" target="_blank" rel="noopener">https://export.arxiv.org/rss/cs.LG</a> ‚Ä¢ Hash: <code>731331997298ba4243c7b13acd28dbf3c0b0f790e9dcfe1d5d27ca15f8701f2a</code></p>
  <table>
    <thead>
      <tr><th>Title</th><th>Summary</th><th>Published</th></tr>
    </thead>
    <tbody>
      <tr><td><a href="https://arxiv.org/abs/2511.07436" target="_blank" rel="noopener">Analysing Environmental Efficiency in AI for X-Ray Diagnosis</a></td><td>arXiv:2511.07436v1 Announce Type: new  Abstract: The integration of AI tools into medical applications has aimed to improve the efficiency of diagnosis. The em‚Ä¶</td><td>2025-11-13T05:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.07437" target="_blank" rel="noopener">Agentic Educational Content Generation for African Languages on Edge Devices</a></td><td>arXiv:2511.07437v1 Announce Type: new  Abstract: Addressing educational inequity in Sub-Saharan Africa, this research presents an autonomous agent-orchestrated‚Ä¶</td><td>2025-11-13T05:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.07483" target="_blank" rel="noopener">Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning</a></td><td>arXiv:2511.07483v1 Announce Type: new  Abstract: Recent advancements in large language models (LLMs) have shifted the post-training paradigm from traditional i‚Ä¶</td><td>2025-11-13T05:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.07568" target="_blank" rel="noopener">Procedural Knowledge Improves Agentic LLM Workflows</a></td><td>arXiv:2511.07568v1 Announce Type: new  Abstract: Large language models (LLMs) often struggle when performing agentic tasks without substantial tool support, pr‚Ä¶</td><td>2025-11-13T05:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.07581" target="_blank" rel="noopener">Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models</a></td><td>arXiv:2511.07581v1 Announce Type: new  Abstract: Effective information retrieval requires reasoning over partial evidence and refining strategies as informatio‚Ä¶</td><td>2025-11-13T05:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.07587" target="_blank" rel="noopener">Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces</a></td><td>arXiv:2511.07587v1 Announce Type: new  Abstract: Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed thei‚Ä¶</td><td>2025-11-13T05:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.07667" target="_blank" rel="noopener">AI-Driven Contribution Evaluation and Conflict Resolution: A Framework &amp; Design for Group Workload Investigation</a></td><td>arXiv:2511.07667v1 Announce Type: new  Abstract: The equitable assessment of individual contribution in teams remains a persistent challenge, where conflict an‚Ä¶</td><td>2025-11-13T05:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.07669" target="_blank" rel="noopener">Making LLMs Reliable When It Matters Most: A Five-Layer Architecture for High-Stakes Decisions</a></td><td>arXiv:2511.07669v1 Announce Type: new  Abstract: Current large language models (LLMs) excel in verifiable domains where outputs can be checked before action bu‚Ä¶</td><td>2025-11-13T05:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.07678" target="_blank" rel="noopener">AIA Forecaster: Technical Report</a></td><td>arXiv:2511.07678v1 Announce Type: new  Abstract: This technical report describes the AIA Forecaster, a Large Language Model (LLM)-based system for judgmental f‚Ä¶</td><td>2025-11-13T05:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.07685" target="_blank" rel="noopener">ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents</a></td><td>arXiv:2511.07685v1 Announce Type: new  Abstract: Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address ope‚Ä¶</td><td>2025-11-13T05:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.07690" target="_blank" rel="noopener">Towards AI-Assisted Generation of Military Training Scenarios</a></td><td>arXiv:2511.07690v1 Announce Type: new  Abstract: Achieving expert-level performance in simulation-based training relies on the creation of complex, adaptable s‚Ä¶</td><td>2025-11-13T05:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.07719" target="_blank" rel="noopener">Operational machine learning for remote spectroscopic detection of CH$_{4}$ point sources</a></td><td>arXiv:2511.07719v1 Announce Type: new  Abstract: Mitigating anthropogenic methane sources is one the most cost-effective levers to slow down global warming. Wh‚Ä¶</td><td>2025-11-13T05:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.07842" target="_blank" rel="noopener">Alignment-Aware Quantization for LLM Safety</a></td><td>arXiv:2511.07842v1 Announce Type: new  Abstract: Safety and efficiency are both important factors when deploying large language models(LLMs). LLMs are trained‚Ä¶</td><td>2025-11-13T05:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.07850" target="_blank" rel="noopener">GAMA: A Neural Neighborhood Search Method with Graph-aware Multi-modal Attention for Vehicle Routing Problem</a></td><td>arXiv:2511.07850v1 Announce Type: new  Abstract: Recent advances in neural neighborhood search methods have shown potential in tackling Vehicle Routing Problem‚Ä¶</td><td>2025-11-13T05:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.07863" target="_blank" rel="noopener">WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking</a></td><td>arXiv:2511.07863v1 Announce Type: new  Abstract: Large language models now draft news, legal analyses, and software code with human-level fluency. At the same‚Ä¶</td><td>2025-11-13T05:00:00Z</td></tr>
    </tbody>
  </table>
</section><section>
  <h2>Hugging Face Trending Models</h2>
  <p class="metadata">Fetched at 2025-10-16T23:53:44Z ‚Ä¢ Source: <a href="https://huggingface.co/api/models" target="_blank" rel="noopener">https://huggingface.co/api/models</a> ‚Ä¢ Hash: <code>edb3d86947c5f8d876d9c4481cb4419a0d2e7876af7b8f691870930c02d571fc</code></p>
  <p class="warning">Illegal header value b&#x27;Bearer &#x27;</p>
  <table>
    <thead>
      <tr><th>Model</th><th>Downloads</th><th>Likes</th></tr>
    </thead>
    <tbody>
      <tr><td><a href="https://huggingface.co/Falconsai/nsfw_image_detection" target="_blank" rel="noopener">Falconsai/nsfw_image_detection</a></td><td>122866637</td><td>847</td></tr>
      <tr><td><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2" target="_blank" rel="noopener">sentence-transformers/all-MiniLM-L6-v2</a></td><td>118061338</td><td>4004</td></tr>
      <tr><td><a href="https://huggingface.co/dima806/fairface_age_image_detection" target="_blank" rel="noopener">dima806/fairface_age_image_detection</a></td><td>81641974</td><td>44</td></tr>
      <tr><td><a href="https://huggingface.co/google/electra-base-discriminator" target="_blank" rel="noopener">google/electra-base-discriminator</a></td><td>58549625</td><td>66</td></tr>
      <tr><td><a href="https://huggingface.co/google-bert/bert-base-uncased" target="_blank" rel="noopener">google-bert/bert-base-uncased</a></td><td>53275807</td><td>2441</td></tr>
      <tr><td><a href="https://huggingface.co/timm/mobilenetv3_small_100.lamb_in1k" target="_blank" rel="noopener">timm/mobilenetv3_small_100.lamb_in1k</a></td><td>43762365</td><td>39</td></tr>
      <tr><td><a href="https://huggingface.co/FacebookAI/roberta-large" target="_blank" rel="noopener">FacebookAI/roberta-large</a></td><td>20426777</td><td>250</td></tr>
      <tr><td><a href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2" target="_blank" rel="noopener">sentence-transformers/all-mpnet-base-v2</a></td><td>17830892</td><td>1170</td></tr>
      <tr><td><a href="https://huggingface.co/openai/clip-vit-base-patch32" target="_blank" rel="noopener">openai/clip-vit-base-patch32</a></td><td>17638896</td><td>782</td></tr>
      <tr><td><a href="https://huggingface.co/pyannote/segmentation-3.0" target="_blank" rel="noopener">pyannote/segmentation-3.0</a></td><td>17234148</td><td>626</td></tr>
      <tr><td><a href="https://huggingface.co/tech4humans/yolov8s-signature-detector" target="_blank" rel="noopener">tech4humans/yolov8s-signature-detector</a></td><td>16874538</td><td>48</td></tr>
      <tr><td><a href="https://huggingface.co/pyannote/wespeaker-voxceleb-resnet34-LM" target="_blank" rel="noopener">pyannote/wespeaker-voxceleb-resnet34-LM</a></td><td>14933122</td><td>80</td></tr>
      <tr><td><a href="https://huggingface.co/Bingsu/adetailer" target="_blank" rel="noopener">Bingsu/adetailer</a></td><td>14489181</td><td>620</td></tr>
      <tr><td><a href="https://huggingface.co/FacebookAI/roberta-base" target="_blank" rel="noopener">FacebookAI/roberta-base</a></td><td>14321545</td><td>529</td></tr>
      <tr><td><a href="https://huggingface.co/pyannote/speaker-diarization-3.1" target="_blank" rel="noopener">pyannote/speaker-diarization-3.1</a></td><td>14186095</td><td>1228</td></tr>
    </tbody>
  </table>
</section><section>
  <h2>GitHub Trending Repositories</h2>
  <p class="metadata">Fetched at 2025-11-13T05:33:08Z ‚Ä¢ Source: <a href="https://api.github.com/search/repositories" target="_blank" rel="noopener">https://api.github.com/search/repositories</a> ‚Ä¢ Hash: <code>511afc20664d24623ecf5dc8d4e43fcd959f173606e3c2a61ee50ed684e1b832</code></p>
  <table>
    <thead>
      <tr><th>Repository</th><th>Stars</th><th>Description</th></tr>
    </thead>
    <tbody>
      <tr><td><a href="https://github.com/ScrapeGraphAI/toonify" target="_blank" rel="noopener">ScrapeGraphAI/toonify</a></td><td>100</td><td>Toonify: Compact data format reducing LLM token usage by 30-60%</td></tr>
      <tr><td><a href="https://github.com/glidea/tinygpt" target="_blank" rel="noopener">glidea/tinygpt</a></td><td>62</td><td>Learning LLM by doing</td></tr>
      <tr><td><a href="https://github.com/EvolvingLMMs-Lab/EASI" target="_blank" rel="noopener">EvolvingLMMs-Lab/EASI</a></td><td>32</td><td>Holistic Evaluation of Multimodal LLMs on Spatial Intelligence</td></tr>
      <tr><td><a href="https://github.com/Shenzhi-Wang/Beyond-the-80-20-Rule-RLVR" target="_blank" rel="noopener">Shenzhi-Wang/Beyond-the-80-20-Rule-RLVR</a></td><td>17</td><td>The open-source code for the NeurIPS 2025 paper, &quot;Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning.&quot;</td></tr>
      <tr><td><a href="https://github.com/zhixiangxue/chak-ai" target="_blank" rel="noopener">zhixiangxue/chak-ai</a></td><td>14</td><td>A simple, yet handy, LLM gateway.</td></tr>
      <tr><td><a href="https://github.com/AIGC-Hackers/mcpx" target="_blank" rel="noopener">AIGC-Hackers/mcpx</a></td><td>13</td><td>Token-efficient MCP client: TypeScript schemas instead of JSON, LLM-friendly syntax, batch calls, TOON output. Built for Claude/GPT automations.</td></tr>
      <tr><td><a href="https://github.com/woshidandan/Assessing-Image-Aesthetics-via-Multimodal-Large-Language-Models" target="_blank" rel="noopener">woshidandan/Assessing-Image-Aesthetics-via-Multimodal-Large-Language-Models</a></td><td>12</td><td>üî•[AAAI 2026, Official Code] Regression Over Classification: Assessing Image Aesthetics via Multimodal Large Language Models. ÂÖãÊúçÂ§ßÊ®°ÂûãÂú®ÁæéÂ≠¶ËØÑ‰º∞ËøáÁ®ã‰∏≠ÂØπÂàÜÊï∞‰∏çÊïèÊÑüÁöÑÈóÆÈ¢ò</td></tr>
      <tr><td><a href="https://github.com/ysyisyourbrother/Jupiter" target="_blank" rel="noopener">ysyisyourbrother/Jupiter</a></td><td>12</td><td>Jupiter is a fast, scalable, and resource-efficient collaborative edge-AI system for generative LLM inference.</td></tr>
      <tr><td><a href="https://github.com/smthemex/ComfyUI_Step_Audio_EditX_SM" target="_blank" rel="noopener">smthemex/ComfyUI_Step_Audio_EditX_SM</a></td><td>10</td><td>Step_Audio_EditXÔºöthe first open-source LLM-based audio model excelling at expressive and iterative audio editing‚Äîencompassing emotion, speaking style, and para‚Ä¶</td></tr>
      <tr><td><a href="https://github.com/AQ-MedAI/MedEQBench" target="_blank" rel="noopener">AQ-MedAI/MedEQBench</a></td><td>8</td><td>The MedEQBench is the first evaluation suite specifically designed for medical contexts to assess Large Language Models&#x27; (LLMs) capabilities in emotional perce‚Ä¶</td></tr>
      <tr><td><a href="https://github.com/P-Bhanu-Sohan/DataVault" target="_blank" rel="noopener">P-Bhanu-Sohan/DataVault</a></td><td>8</td><td>HACK UMASS XIII - Winner!: Best use of Vultr Cloud. A framework to allow training of LLMs on sensitive data while maintaining privacy</td></tr>
      <tr><td><a href="https://github.com/limyewjin/llm-bash" target="_blank" rel="noopener">limyewjin/llm-bash</a></td><td>7</td><td>A Bash framework following UNIX philosophy for building LLM-powered agent workflows with prompt chaining, orchestration, and intelligent routing patterns.</td></tr>
      <tr><td><a href="https://github.com/olaflaitinen/llm-proteomics-hallucination" target="_blank" rel="noopener">olaflaitinen/llm-proteomics-hallucination</a></td><td>7</td><td>Systematic evaluation of hallucination risks in Large Language Models (GPT-4, Claude 3, Gemini Pro) for clinical proteomics and mass spectrometry interpretatio‚Ä¶</td></tr>
      <tr><td><a href="https://github.com/FeiLiu36/EoH-S" target="_blank" rel="noopener">FeiLiu36/EoH-S</a></td><td>6</td><td>Evolution of Heuristic Set using LLMs for Automated Heuristic Design</td></tr>
      <tr><td><a href="https://github.com/roguetrainer/functional-programming-in-llm-interactions" target="_blank" rel="noopener">roguetrainer/functional-programming-in-llm-interactions</a></td><td>6</td><td>Functional programming and category theory in LLM interactions</td></tr>
      <tr><td><a href="https://github.com/ultralytics/llm" target="_blank" rel="noopener">ultralytics/llm</a></td><td>6</td><td>Ultralytics LLM-related experiments</td></tr>
      <tr><td><a href="https://github.com/williamrhancock/Interflow" target="_blank" rel="noopener">williamrhancock/Interflow</a></td><td>6</td><td>A visual chained LLM response system, allowing you to drill in to specifics within the LLM response.</td></tr>
      <tr><td><a href="https://github.com/Dicklesworthstone/markdown_web_browser" target="_blank" rel="noopener">Dicklesworthstone/markdown_web_browser</a></td><td>5</td><td>A web browser for LLMs that automatically turns every page into rich markdown</td></tr>
      <tr><td><a href="https://github.com/YXNTU/EHRStruct" target="_blank" rel="noopener">YXNTU/EHRStruct</a></td><td>5</td><td>This repository contains the official implementation of the paper: &quot;EHRStruct: A Comprehensive Benchmark Framework for Evaluating Large Language Models on Stru‚Ä¶</td></tr>
      <tr><td><a href="https://github.com/aibrix/PrisKV" target="_blank" rel="noopener">aibrix/PrisKV</a></td><td>5</td><td>High Performance KV Cache Store for LLM</td></tr>
    </tbody>
  </table>
</section><section>
  <h2>Papers with Code</h2>
  <p class="metadata">Fetched at 2025-10-17T00:47:37Z ‚Ä¢ Source: <a href="https://paperswithcode.com/api/v1/papers/" target="_blank" rel="noopener">https://paperswithcode.com/api/v1/papers/</a> ‚Ä¢ Hash: <code>d4a343f7408e936b9b783882d60713e7711a2e2104b924d1a9726c2b1660e60c</code></p>
  <p class="warning">Expecting value: line 1 column 1 (char 0)</p>
  <p class="empty">No data available.</p>
</section><section>
  <h2>Hacker News Highlights</h2>
  <p class="metadata">Fetched at 2025-11-13T05:33:08Z ‚Ä¢ Source: <a href="https://hacker-news.firebaseio.com/" target="_blank" rel="noopener">https://hacker-news.firebaseio.com/</a> ‚Ä¢ Hash: <code>8a220b0c5c79c7a3ebb71f31b190d2cb7fdd71211e22b3a158a505b1d5b978ab</code></p>
  <table>
    <thead>
      <tr><th>Story</th><th>Score</th><th>Published</th></tr>
    </thead>
    <tbody>
      <tr><td><a href="https://www.nasdaq.com/articles/metas-chief-ai-scientist-yann-lecun-depart-and-launch-ai-start-focused-world-models" target="_blank" rel="noopener">Yann LeCun to depart Meta and launch AI startup focused on &#x27;world models&#x27;</a></td><td>808</td><td>2025-11-12T07:25:30Z</td></tr>
      <tr><td><a href="https://twitter.com/omar_quraishi/status/1988518627859951986" target="_blank" rel="noopener">Pakistani newspaper mistakenly prints AI prompt with the article</a></td><td>477</td><td>2025-11-12T11:17:06Z</td></tr>
      <tr><td><a href="https://openai.com/index/gpt-5-1/" target="_blank" rel="noopener">GPT-5.1: A smarter, more conversational ChatGPT</a></td><td>247</td><td>2025-11-12T19:05:41Z</td></tr>
      <tr><td><a href="https://www.tomshardware.com/pc-components/hdds/ai-triggers-hard-drive-shortage-amidst-dram-squeeze-enterprise-hard-drives-on-backorder-by-2-years-as-hyperscalers-switch-to-qlc-ssds" target="_blank" rel="noopener">Hard drives on backorder for two years as AI data centers trigger HDD shortage</a></td><td>192</td><td>2025-11-12T05:36:41Z</td></tr>
      <tr><td><a href="https://www.worldlabs.ai/blog/marble-world-model" target="_blank" rel="noopener">Marble: A Multimodal World Model</a></td><td>153</td><td>2025-11-12T22:11:14Z</td></tr>
      <tr><td><a href="https://www.anthropic.com/news/anthropic-invests-50-billion-in-american-ai-infrastructure" target="_blank" rel="noopener">Anthropic invests $50B in US AI infrastructure</a></td><td>102</td><td>2025-11-12T15:42:35Z</td></tr>
      <tr><td><a href="http://marble.worldlabs.ai/" target="_blank" rel="noopener">Marble by World Labs: Multimodal world model to create and edit 3D worlds</a></td><td>44</td><td>2025-11-12T17:13:30Z</td></tr>
      <tr><td><a href="https://www.ft.com/content/fce77ba4-6231-4920-9e99-693a6c38e7d5" target="_blank" rel="noopener">How high are OpenAI&#x27;s compute costs? Possibly a lot higher than we thought</a></td><td>33</td><td>2025-11-12T19:41:35Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.07585" target="_blank" rel="noopener">LLM Output Drift in Financial Workflows: Validation and Mitigation (arXiv)</a></td><td>22</td><td>2025-11-12T19:53:25Z</td></tr>
      <tr><td><a href="https://www.theregister.com/2025/11/12/openai_spending_report/" target="_blank" rel="noopener">OpenAI&#x27;s viability called into question by reported spending with Microsoft</a></td><td>22</td><td>2025-11-12T21:23:40Z</td></tr>
      <tr><td><a href="https://www.theatlantic.com/technology/2025/11/openai-lawsuit-subpoenas/684861/" target="_blank" rel="noopener">The New Brutality of OpenAI</a></td><td>13</td><td>2025-11-13T04:31:02Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2511.06221" target="_blank" rel="noopener">Tiny Model, Big Logic: Large-Model Reasoning Ability in VibeThinker-1.5B</a></td><td>3</td><td>2025-11-12T16:19:30Z</td></tr>
    </tbody>
  </table>
</section><section>
  <h2>Latest CVEs</h2>
  <p class="metadata">Fetched at 2025-11-13T05:33:08Z ‚Ä¢ Source: <a href="https://cve.circl.lu/api/last" target="_blank" rel="noopener">https://cve.circl.lu/api/last</a> ‚Ä¢ Hash: <code>78aa07fb3bde7b004ea63055169354d102ace2c5c7834710a0bb2b2d658068b0</code></p>
  <table>
    <thead>
      <tr><th>CVE</th><th>CVSS</th><th>Summary</th></tr>
    </thead>
    <tbody>
      <tr><td>N/A</td><td>0.0</td><td></td></tr>
      <tr><td>N/A</td><td>0.0</td><td></td></tr>
      <tr><td>N/A</td><td>0.0</td><td></td></tr>
      <tr><td>N/A</td><td>0.0</td><td></td></tr>
      <tr><td>N/A</td><td>0.0</td><td></td></tr>
      <tr><td>N/A</td><td>0.0</td><td></td></tr>
      <tr><td>N/A</td><td>0.0</td><td></td></tr>
      <tr><td>N/A</td><td>0.0</td><td></td></tr>
      <tr><td>N/A</td><td>0.0</td><td></td></tr>
      <tr><td>N/A</td><td>0.0</td><td></td></tr>
    </tbody>
  </table>
</section><section>
  <h2>Hugging Face Trending Datasets</h2>
  <p class="metadata">Fetched at 2025-10-16T23:53:44Z ‚Ä¢ Source: <a href="https://huggingface.co/api/datasets" target="_blank" rel="noopener">https://huggingface.co/api/datasets</a> ‚Ä¢ Hash: <code>4b4cc33871c4b3cd497c500155aa79f37885aa24df2d895e1d18c0c80ab6ae9c</code></p>
  <p class="warning">Illegal header value b&#x27;Bearer &#x27;</p>
  <table>
    <thead>
      <tr><th>Dataset</th><th>Downloads</th><th>Likes</th></tr>
    </thead>
    <tbody>
      <tr><td><a href="https://huggingface.co/datasets/nebius/SWE-rebench" target="_blank" rel="noopener">nebius/SWE-rebench</a></td><td>3717252</td><td>35</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/SWE-Gym/SWE-Gym" target="_blank" rel="noopener">SWE-Gym/SWE-Gym</a></td><td>2041687</td><td>20</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/huggingface/documentation-images" target="_blank" rel="noopener">huggingface/documentation-images</a></td><td>2014781</td><td>86</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/chcorbi/helvipad" target="_blank" rel="noopener">chcorbi/helvipad</a></td><td>1478443</td><td>10</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/AquaV/genshin-voices-separated" target="_blank" rel="noopener">AquaV/genshin-voices-separated</a></td><td>1461805</td><td>8</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/banned-historical-archives/banned-historical-archives" target="_blank" rel="noopener">banned-historical-archives/banned-historical-archives</a></td><td>1411450</td><td>5</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim" target="_blank" rel="noopener">nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim</a></td><td>1339503</td><td>155</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified" target="_blank" rel="noopener">princeton-nlp/SWE-bench_Verified</a></td><td>1282494</td><td>218</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/lavita/medical-qa-shared-task-v1-toy" target="_blank" rel="noopener">lavita/medical-qa-shared-task-v1-toy</a></td><td>878541</td><td>21</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/Salesforce/wikitext" target="_blank" rel="noopener">Salesforce/wikitext</a></td><td>867174</td><td>505</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/IPEC-COMMUNITY/language_table_lerobot" target="_blank" rel="noopener">IPEC-COMMUNITY/language_table_lerobot</a></td><td>786287</td><td>0</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/IPEC-COMMUNITY/droid_lerobot" target="_blank" rel="noopener">IPEC-COMMUNITY/droid_lerobot</a></td><td>737114</td><td>6</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/hf-doc-build/doc-build" target="_blank" rel="noopener">hf-doc-build/doc-build</a></td><td>671848</td><td>12</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/applied-ai-018/pretraining_v1-omega_books" target="_blank" rel="noopener">applied-ai-018/pretraining_v1-omega_books</a></td><td>614296</td><td>2</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu" target="_blank" rel="noopener">HuggingFaceFW/fineweb-edu</a></td><td>597646</td><td>775</td></tr>
    </tbody>
  </table>
</section>
  </main>
  <footer><p>Datasets updated as part of the DeepTech Daily automation run.</p><p>Last refreshed at 2025-11-13T05:33:08Z.</p><ul><li><a href="../deeptech-daily/data/gpu_prices.json">gpu_prices.json</a></li><li><a href="../deeptech-daily/data/arxiv.yaml">arxiv.yaml</a></li><li><a href="../deeptech-daily/data/hf_trending.yaml">hf_trending.yaml</a></li><li><a href="../deeptech-daily/data/github_trending.yaml">github_trending.yaml</a></li><li><a href="../deeptech-daily/data/pwc_llm.yaml">pwc_llm.yaml</a></li><li><a href="../deeptech-daily/data/hn_ai.yaml">hn_ai.yaml</a></li><li><a href="../deeptech-daily/data/cves.yaml">cves.yaml</a></li><li><a href="../deeptech-daily/data/hf_datasets.yaml">hf_datasets.yaml</a></li></ul></footer>
</body>
</html>
