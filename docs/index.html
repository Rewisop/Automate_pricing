<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>DeepTech Daily Dashboard</title>
  <style>
body { font-family: 'Inter', 'Segoe UI', system-ui, -apple-system, sans-serif; margin: 0; padding: 0; background: #0f172a; color: #e2e8f0; }
header { padding: 2.5rem 1.5rem; text-align: center; background: #1e293b; }
header h1 { margin: 0 0 0.5rem; font-size: 2.5rem; }
header p { margin: 0; color: #94a3b8; font-size: 1rem; }
main { max-width: 1100px; margin: 0 auto; padding: 2rem 1.5rem 4rem; }
section { background: rgba(15, 23, 42, 0.75); border: 1px solid rgba(148, 163, 184, 0.2); border-radius: 12px; padding: 1.5rem; margin-bottom: 2rem; box-shadow: 0 12px 24px rgba(15, 23, 42, 0.4); }
section h2 { margin-top: 0; color: #f8fafc; font-size: 1.5rem; }
table { width: 100%; border-collapse: collapse; margin-top: 1rem; }
th, td { text-align: left; padding: 0.65rem 0.75rem; border-bottom: 1px solid rgba(148, 163, 184, 0.2); }
th { background: rgba(148, 163, 184, 0.1); color: #e2e8f0; font-weight: 600; }
tr:hover td { background: rgba(59, 130, 246, 0.08); }
a { color: #38bdf8; text-decoration: none; }
a:hover { text-decoration: underline; }
.warning { margin: 0.75rem 0; color: #fbbf24; }
.empty { margin: 1rem 0 0; color: #94a3b8; font-style: italic; }
.metadata { margin: 0.35rem 0 0; color: #94a3b8; font-size: 0.9rem; }
footer { text-align: center; padding: 2rem 1.5rem 3rem; color: #64748b; font-size: 0.9rem; }
footer ul { list-style: none; padding: 0; margin: 1rem 0 0; display: flex; flex-wrap: wrap; gap: 0.75rem; justify-content: center; }
footer li { background: rgba(148, 163, 184, 0.1); border-radius: 6px; padding: 0.4rem 0.75rem; }
</style>
</head>
<body>
  <header>
    <h1>DeepTech Daily Dashboard</h1>
    <p>A curated snapshot of GPUs, research, software, and security intel powered by the DeepTech Daily datasets.</p>
  </header>
  <main>
    <section>
  <h2>GPU Pricing Snapshot</h2>
  <p class="metadata">Fetched at 2025-10-17T00:47:37Z • Source: <a href="https://vast.ai/api/v0/bundles/public" target="_blank" rel="noopener">https://vast.ai/api/v0/bundles/public</a>, <a href="https://api.runpod.io" target="_blank" rel="noopener">https://api.runpod.io</a>, <a href="https://modal.com/api" target="_blank" rel="noopener">https://modal.com/api</a> • Hash: <code>73cd0d24819c38ed4bb8d48de66e8dd1013b8b60870640a96bc6c90d3b6d6974</code></p>
  <p class="warning">Vast.ai: 404 Client Error: Not Found for url: https://console.vast.ai/api/v0/bundles/public/?limit=200&amp;skip=0</p>
  <p class="empty">No data available.</p>
</section><section>
  <h2>arXiv Digest</h2>
  <p class="metadata">Fetched at 2025-10-27T05:33:40Z • Source: <a href="https://export.arxiv.org/rss/cs.AI" target="_blank" rel="noopener">https://export.arxiv.org/rss/cs.AI</a>, <a href="https://export.arxiv.org/rss/cs.CL" target="_blank" rel="noopener">https://export.arxiv.org/rss/cs.CL</a>, <a href="https://export.arxiv.org/rss/cs.LG" target="_blank" rel="noopener">https://export.arxiv.org/rss/cs.LG</a> • Hash: <code>bf48155e42c340d6ccf27e74ab53d0cb1e143556769a70311f6210ca38b38f06</code></p>
  <table>
    <thead>
      <tr><th>Title</th><th>Summary</th><th>Published</th></tr>
    </thead>
    <tbody>
      <tr><td><a href="https://arxiv.org/abs/2510.20838" target="_blank" rel="noopener">Sketch2BIM: A Multi-Agent Human-AI Collaborative Pipeline to Convert Hand-Drawn Floor Plans to 3D BIM</a></td><td>arXiv:2510.20838v1 Announce Type: new  Abstract: This study introduces a human-in-the-loop pipeline that converts unscaled, hand-drawn floor plan sketches into…</td><td>2025-10-27T04:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2510.20849" target="_blank" rel="noopener">Cultural Alien Sampler: Open-ended art generation balancing originality and coherence</a></td><td>arXiv:2510.20849v1 Announce Type: new  Abstract: In open-ended domains like art, autonomous agents must generate ideas that are both original and internally co…</td><td>2025-10-27T04:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2510.20861" target="_blank" rel="noopener">Fuzzy numbers revisited: operations on extensional fuzzy numbers</a></td><td>arXiv:2510.20861v1 Announce Type: new  Abstract: Fuzzy numbers are commonly represented with fuzzy sets. Their objective is to better represent imprecise data.…</td><td>2025-10-27T04:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2510.21027" target="_blank" rel="noopener">Customizing Open Source LLMs for Quantitative Medication Attribute Extraction across Heterogeneous EHR Systems</a></td><td>arXiv:2510.21027v1 Announce Type: new  Abstract: Harmonizing medication data across Electronic Health Record (EHR) systems is a persistent barrier to monitorin…</td><td>2025-10-27T04:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2510.21043" target="_blank" rel="noopener">Epistemic Deference to AI</a></td><td>arXiv:2510.21043v1 Announce Type: new  Abstract: When should we defer to AI outputs over human expert judgment? Drawing on recent work in social epistemology,…</td><td>2025-10-27T04:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2510.21045" target="_blank" rel="noopener">From Questions to Queries: An AI-powered Multi-Agent Framework for Spatial Text-to-SQL</a></td><td>arXiv:2510.21045v1 Announce Type: new  Abstract: The complexity of Structured Query Language (SQL) and the specialized nature of geospatial functions in tools…</td><td>2025-10-27T04:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2510.21093" target="_blank" rel="noopener">MedAlign: A Synergistic Framework of Multimodal Preference Optimization and Federated Meta-Cognitive Reasoning</a></td><td>arXiv:2510.21093v1 Announce Type: new  Abstract: Recently, large models have shown significant potential for smart healthcare. However, the deployment of Large…</td><td>2025-10-27T04:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2510.21110" target="_blank" rel="noopener">Confounding Robust Deep Reinforcement Learning: A Causal Approach</a></td><td>arXiv:2510.21110v1 Announce Type: new  Abstract: A key task in Artificial Intelligence is learning effective policies for controlling agents in unknown environ…</td><td>2025-10-27T04:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2510.21117" target="_blank" rel="noopener">DAO-AI: Evaluating Collective Decision-Making through Agentic AI in Decentralized Governance</a></td><td>arXiv:2510.21117v1 Announce Type: new  Abstract: This paper presents a first empirical study of agentic AI as autonomous decision-makers in decentralized gover…</td><td>2025-10-27T04:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2510.21143" target="_blank" rel="noopener">PanicToCalm: A Proactive Counseling Agent for Panic Attacks</a></td><td>arXiv:2510.21143v1 Announce Type: new  Abstract: Panic attacks are acute episodes of fear and distress, in which timely, appropriate intervention can significa…</td><td>2025-10-27T04:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2510.21144" target="_blank" rel="noopener">NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge</a></td><td>arXiv:2510.21144v1 Announce Type: new  Abstract: Retrieval-Augmented Generation (RAG) empowers Large Language Models (LLMs) to dynamically integrate external k…</td><td>2025-10-27T04:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2510.21148" target="_blank" rel="noopener">How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation</a></td><td>arXiv:2510.21148v1 Announce Type: new  Abstract: Designing optimal prompts and reasoning processes for large language models (LLMs) on domain-specific tasks is…</td><td>2025-10-27T04:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2510.21150" target="_blank" rel="noopener">String Seed of Thought: Prompting LLMs for Distribution-Faithful and Diverse Generation</a></td><td>arXiv:2510.21150v1 Announce Type: new  Abstract: We introduce String Seed of Thought (SSoT), a novel prompting method for LLMs that improves Probabilistic Inst…</td><td>2025-10-27T04:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2510.21175" target="_blank" rel="noopener">Memory-Free Continual Learning with Null Space Adaptation for Zero-Shot Vision-Language Models</a></td><td>arXiv:2510.21175v1 Announce Type: new  Abstract: Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated remarkable zero-shot generalization…</td><td>2025-10-27T04:00:00Z</td></tr>
      <tr><td><a href="https://arxiv.org/abs/2510.21181" target="_blank" rel="noopener">Shylock: Causal Discovery in Multivariate Time Series based on Hybrid Constraints</a></td><td>arXiv:2510.21181v1 Announce Type: new  Abstract: Causal relationship discovery has been drawing increasing attention due to its prevalent application. Existing…</td><td>2025-10-27T04:00:00Z</td></tr>
    </tbody>
  </table>
</section><section>
  <h2>Hugging Face Trending Models</h2>
  <p class="metadata">Fetched at 2025-10-16T23:53:44Z • Source: <a href="https://huggingface.co/api/models" target="_blank" rel="noopener">https://huggingface.co/api/models</a> • Hash: <code>edb3d86947c5f8d876d9c4481cb4419a0d2e7876af7b8f691870930c02d571fc</code></p>
  <p class="warning">401 Client Error: Unauthorized for url: https://huggingface.co/api/models?sort=downloads&amp;direction=-1&amp;limit=15 (Request ID: Root=1-68ff043c-0e3724a5380c743b2a13cde5;287bbc61-8b53-4965-944e-d39e591ed811) Invalid credentials in Authorization header</p>
  <table>
    <thead>
      <tr><th>Model</th><th>Downloads</th><th>Likes</th></tr>
    </thead>
    <tbody>
      <tr><td><a href="https://huggingface.co/Falconsai/nsfw_image_detection" target="_blank" rel="noopener">Falconsai/nsfw_image_detection</a></td><td>122866637</td><td>847</td></tr>
      <tr><td><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2" target="_blank" rel="noopener">sentence-transformers/all-MiniLM-L6-v2</a></td><td>118061338</td><td>4004</td></tr>
      <tr><td><a href="https://huggingface.co/dima806/fairface_age_image_detection" target="_blank" rel="noopener">dima806/fairface_age_image_detection</a></td><td>81641974</td><td>44</td></tr>
      <tr><td><a href="https://huggingface.co/google/electra-base-discriminator" target="_blank" rel="noopener">google/electra-base-discriminator</a></td><td>58549625</td><td>66</td></tr>
      <tr><td><a href="https://huggingface.co/google-bert/bert-base-uncased" target="_blank" rel="noopener">google-bert/bert-base-uncased</a></td><td>53275807</td><td>2441</td></tr>
      <tr><td><a href="https://huggingface.co/timm/mobilenetv3_small_100.lamb_in1k" target="_blank" rel="noopener">timm/mobilenetv3_small_100.lamb_in1k</a></td><td>43762365</td><td>39</td></tr>
      <tr><td><a href="https://huggingface.co/FacebookAI/roberta-large" target="_blank" rel="noopener">FacebookAI/roberta-large</a></td><td>20426777</td><td>250</td></tr>
      <tr><td><a href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2" target="_blank" rel="noopener">sentence-transformers/all-mpnet-base-v2</a></td><td>17830892</td><td>1170</td></tr>
      <tr><td><a href="https://huggingface.co/openai/clip-vit-base-patch32" target="_blank" rel="noopener">openai/clip-vit-base-patch32</a></td><td>17638896</td><td>782</td></tr>
      <tr><td><a href="https://huggingface.co/pyannote/segmentation-3.0" target="_blank" rel="noopener">pyannote/segmentation-3.0</a></td><td>17234148</td><td>626</td></tr>
      <tr><td><a href="https://huggingface.co/tech4humans/yolov8s-signature-detector" target="_blank" rel="noopener">tech4humans/yolov8s-signature-detector</a></td><td>16874538</td><td>48</td></tr>
      <tr><td><a href="https://huggingface.co/pyannote/wespeaker-voxceleb-resnet34-LM" target="_blank" rel="noopener">pyannote/wespeaker-voxceleb-resnet34-LM</a></td><td>14933122</td><td>80</td></tr>
      <tr><td><a href="https://huggingface.co/Bingsu/adetailer" target="_blank" rel="noopener">Bingsu/adetailer</a></td><td>14489181</td><td>620</td></tr>
      <tr><td><a href="https://huggingface.co/FacebookAI/roberta-base" target="_blank" rel="noopener">FacebookAI/roberta-base</a></td><td>14321545</td><td>529</td></tr>
      <tr><td><a href="https://huggingface.co/pyannote/speaker-diarization-3.1" target="_blank" rel="noopener">pyannote/speaker-diarization-3.1</a></td><td>14186095</td><td>1228</td></tr>
    </tbody>
  </table>
</section><section>
  <h2>GitHub Trending Repositories</h2>
  <p class="metadata">Fetched at 2025-10-27T05:33:40Z • Source: <a href="https://api.github.com/search/repositories" target="_blank" rel="noopener">https://api.github.com/search/repositories</a> • Hash: <code>bfa923f3213c5a6e1c7374a2d8941bf7ece6f3db700d80dc9567d90b788fab8f</code></p>
  <table>
    <thead>
      <tr><th>Repository</th><th>Stars</th><th>Description</th></tr>
    </thead>
    <tbody>
      <tr><td><a href="https://github.com/pguso/ai-agents-from-scratch" target="_blank" rel="noopener">pguso/ai-agents-from-scratch</a></td><td>716</td><td>Demystify AI agents by building them yourself. Local LLMs, no black boxes, real understanding of function calling, memory, and ReAct patterns.</td></tr>
      <tr><td><a href="https://github.com/johannschopplich/toon" target="_blank" rel="noopener">johannschopplich/toon</a></td><td>507</td><td>🎒 Token-Oriented Object Notation – JSON for LLMs at half the token cost</td></tr>
      <tr><td><a href="https://github.com/WooooDyy/BAPO" target="_blank" rel="noopener">WooooDyy/BAPO</a></td><td>65</td><td>Codes for the paper &quot;BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping&quot;  by Zhiheng Xi et al.</td></tr>
      <tr><td><a href="https://github.com/barodeur/llm_rescuer" target="_blank" rel="noopener">barodeur/llm_rescuer</a></td><td>56</td><td>Fix the Billion Dollar Mistake with an LLM</td></tr>
      <tr><td><a href="https://github.com/panozzaj/plain_errors" target="_blank" rel="noopener">panozzaj/plain_errors</a></td><td>55</td><td>Rails middleware for LLM-optimized error messages</td></tr>
      <tr><td><a href="https://github.com/OPPO-Mente-Lab/DaMo" target="_blank" rel="noopener">OPPO-Mente-Lab/DaMo</a></td><td>26</td><td>The official implement of paper 《DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents》</td></tr>
      <tr><td><a href="https://github.com/onyx-dot-app/onyx-foss" target="_blank" rel="noopener">onyx-dot-app/onyx-foss</a></td><td>21</td><td>Open Source AI Platform - AI Chat with advanced features that works with every LLM</td></tr>
      <tr><td><a href="https://github.com/Prat011/awesome-llm-skills" target="_blank" rel="noopener">Prat011/awesome-llm-skills</a></td><td>20</td><td>A curated list of awesome LLM Skills, resources and tools for customising LLM workflows - that works with Claude Code, Codex, Gemini CLI, Qwen Code, OpenCode e…</td></tr>
      <tr><td><a href="https://github.com/adamgallas/llama-fpga" target="_blank" rel="noopener">adamgallas/llama-fpga</a></td><td>19</td><td>[DATE&#x27;25, ICCAD&#x27;25] An embedded FPGA-based LLM accelerator capable of supporting Llama2-7B</td></tr>
      <tr><td><a href="https://github.com/openguardrails/openguardrails" target="_blank" rel="noopener">openguardrails/openguardrails</a></td><td>19</td><td>Adding guardrails to large language models.</td></tr>
      <tr><td><a href="https://github.com/zlab-princeton/llm-distillation-jax" target="_blank" rel="noopener">zlab-princeton/llm-distillation-jax</a></td><td>12</td><td>JAX implementation of configurable LLM distillation training</td></tr>
      <tr><td><a href="https://github.com/kristaller486/RuQualBench" target="_blank" rel="noopener">kristaller486/RuQualBench</a></td><td>11</td><td>RuQualBench: A benchmark for evaluating the quality of the Russian language in LLM responses</td></tr>
      <tr><td><a href="https://github.com/LetterLiGo/Agent-WebCloak" target="_blank" rel="noopener">LetterLiGo/Agent-WebCloak</a></td><td>9</td><td>[IEEE S&amp;P&#x27;26] WebCloak: Characterizing and Mitigating the Threats of LLM-Driven Web Agents as Intelligent Scrapers</td></tr>
      <tr><td><a href="https://github.com/SanBingYouYong/shapecraft" target="_blank" rel="noopener">SanBingYouYong/shapecraft</a></td><td>9</td><td>Official repo for NeurIPS 2025 poster - ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling.</td></tr>
      <tr><td><a href="https://github.com/ai-28/Scalable-RAG-in-AWS-with-Fargate" target="_blank" rel="noopener">ai-28/Scalable-RAG-in-AWS-with-Fargate</a></td><td>9</td><td>This repository contains a full RAG application using Terraform as IaC, LangChain as framework, AWS Bedrock as LLM and Embedding Models, AWS OpenSearch as a ve…</td></tr>
      <tr><td><a href="https://github.com/Infrared1029/simpleprompts" target="_blank" rel="noopener">Infrared1029/simpleprompts</a></td><td>8</td><td>A simple library for constructing LLM prompts</td></tr>
      <tr><td><a href="https://github.com/yanhong-lbh/text_or_pixels" target="_blank" rel="noopener">yanhong-lbh/text_or_pixels</a></td><td>8</td><td>Codebase for EMNLP 2025 Findings paper &quot;Text or Pixels? Evaluating Efficiency and Understanding of LLMs with Visual Text Inputs&quot;</td></tr>
      <tr><td><a href="https://github.com/D1aoBoomm/LoRO" target="_blank" rel="noopener">D1aoBoomm/LoRO</a></td><td>7</td><td>Code for implementing &quot;LoRO: Real-Time on-Device Secure Inference for LLMs via TEE-Based Low Rank Obfuscation&quot; (NeurIPS 2025)</td></tr>
      <tr><td><a href="https://github.com/NuyoahCh/LLMForge" target="_blank" rel="noopener">NuyoahCh/LLMForge</a></td><td>6</td><td>一份面向开发者的开源教程，用于系统学习如何构建、部署和优化基于大语言模型（LLM）的智能应用。</td></tr>
      <tr><td><a href="https://github.com/kyegomez/Open-Cursor-Agent" target="_blank" rel="noopener">kyegomez/Open-Cursor-Agent</a></td><td>6</td><td>An open-source autonomous AI agent implementation inspired by Cursor Agent, built with the Swarms framework. This production-grade agent can autonomously plan,…</td></tr>
    </tbody>
  </table>
</section><section>
  <h2>Papers with Code</h2>
  <p class="metadata">Fetched at 2025-10-17T00:47:37Z • Source: <a href="https://paperswithcode.com/api/v1/papers/" target="_blank" rel="noopener">https://paperswithcode.com/api/v1/papers/</a> • Hash: <code>d4a343f7408e936b9b783882d60713e7711a2e2104b924d1a9726c2b1660e60c</code></p>
  <p class="warning">Expecting value: line 1 column 1 (char 0)</p>
  <p class="empty">No data available.</p>
</section><section>
  <h2>Hacker News Highlights</h2>
  <p class="metadata">Fetched at 2025-10-27T05:33:40Z • Source: <a href="https://hacker-news.firebaseio.com/" target="_blank" rel="noopener">https://hacker-news.firebaseio.com/</a> • Hash: <code>41b8d4085198720d6ea450d7ae80413658635cef7a9d836421db5cd089e654c1</code></p>
  <table>
    <thead>
      <tr><th>Story</th><th>Score</th><th>Published</th></tr>
    </thead>
    <tbody>
      <tr><td><a href="https://jacobin.com/2025/10/ice-zignal-surveillance-social-media" target="_blank" rel="noopener">ICE Will Use AI to Surveil Social Media</a></td><td>159</td><td>2025-10-27T00:43:32Z</td></tr>
      <tr><td><a href="https://dipakwani.com/ai-mafia/" target="_blank" rel="noopener">AI Mafia Network – An interactive visualization</a></td><td>92</td><td>2025-10-26T22:54:23Z</td></tr>
      <tr><td><a href="https://booksbypeople.org/" target="_blank" rel="noopener">Books by People – Defending Organic Literature in an AI World</a></td><td>63</td><td>2025-10-26T16:57:44Z</td></tr>
      <tr><td><a href="https://github.com/theaniketgiri/create-llm" target="_blank" rel="noopener">Show HN: Create-LLM – Train your own LLM in 60 seconds</a></td><td>41</td><td>2025-10-26T09:53:24Z</td></tr>
      <tr><td><a href="https://www.planetearthandbeyond.co/p/you-have-no-idea-how-screwed-openai" target="_blank" rel="noopener">You Have No Idea How Screwed OpenAI Is</a></td><td>21</td><td>2025-10-26T22:23:06Z</td></tr>
      <tr><td><a href="https://techcrunch.com/2025/10/25/the-glaring-security-risks-with-ai-browser-agents/" target="_blank" rel="noopener">The glaring security risks with AI browser agents</a></td><td>12</td><td>2025-10-26T19:33:11Z</td></tr>
      <tr><td><a href="https://www.msn.com/en-us/news/us/ar-AA1Pavwq" target="_blank" rel="noopener">Student handcuffed after Doritos bag mistaken for a gun by AI security system</a></td><td>10</td><td>2025-10-26T22:09:56Z</td></tr>
      <tr><td><a href="https://fluxwing.com" target="_blank" rel="noopener">Fluxwing: ASCII-first UX design system with derivation model</a></td><td>7</td><td>2025-10-27T00:07:51Z</td></tr>
      <tr><td><a href="https://www.theguardian.com/technology/2025/oct/27/labor-rules-out-giving-tech-giants-free-rein-to-mine-copyright-content-to-train-ai" target="_blank" rel="noopener">Oz Labor Gov rules out giving tech giants free rein to mine IPR to train AI</a></td><td>4</td><td>2025-10-27T00:45:51Z</td></tr>
    </tbody>
  </table>
</section><section>
  <h2>Latest CVEs</h2>
  <p class="metadata">Fetched at 2025-10-27T05:33:40Z • Source: <a href="https://cve.circl.lu/api/last" target="_blank" rel="noopener">https://cve.circl.lu/api/last</a> • Hash: <code>73f73a57312f91235a70357cc2c6f7268fd79c4ac250c380ddd49fc0fe220a09</code></p>
  <table>
    <thead>
      <tr><th>CVE</th><th>CVSS</th><th>Summary</th></tr>
    </thead>
    <tbody>
      <tr><td><a href="https://www.cve.org/CVERecord?id=MAL-2025-48609" target="_blank" rel="noopener">MAL-2025-48609</a></td><td>0.0</td><td>Malicious code in ember-simplepractice (npm)</td></tr>
      <tr><td><a href="https://www.cve.org/CVERecord?id=GHSA-p98w-mx82-xhg4" target="_blank" rel="noopener">GHSA-p98w-mx82-xhg4</a></td><td>0.0</td><td></td></tr>
      <tr><td><a href="https://www.cve.org/CVERecord?id=GHSA-p9x7-3xvx-2h65" target="_blank" rel="noopener">GHSA-p9x7-3xvx-2h65</a></td><td>0.0</td><td></td></tr>
      <tr><td><a href="https://www.cve.org/CVERecord?id=GHSA-prw8-6vx3-vrxm" target="_blank" rel="noopener">GHSA-prw8-6vx3-vrxm</a></td><td>0.0</td><td></td></tr>
      <tr><td><a href="https://www.cve.org/CVERecord?id=GHSA-qc8p-jp3g-x9hm" target="_blank" rel="noopener">GHSA-qc8p-jp3g-x9hm</a></td><td>0.0</td><td></td></tr>
      <tr><td><a href="https://www.cve.org/CVERecord?id=GHSA-qgvx-wfx5-x9qg" target="_blank" rel="noopener">GHSA-qgvx-wfx5-x9qg</a></td><td>0.0</td><td></td></tr>
      <tr><td><a href="https://www.cve.org/CVERecord?id=GHSA-qvpv-xh75-74pw" target="_blank" rel="noopener">GHSA-qvpv-xh75-74pw</a></td><td>0.0</td><td></td></tr>
      <tr><td><a href="https://www.cve.org/CVERecord?id=GHSA-r3xr-wm72-mg2w" target="_blank" rel="noopener">GHSA-r3xr-wm72-mg2w</a></td><td>0.0</td><td></td></tr>
      <tr><td><a href="https://www.cve.org/CVERecord?id=GHSA-rj3x-qp7v-5vp5" target="_blank" rel="noopener">GHSA-rj3x-qp7v-5vp5</a></td><td>0.0</td><td></td></tr>
      <tr><td><a href="https://www.cve.org/CVERecord?id=GHSA-rqm5-mx2c-5fhp" target="_blank" rel="noopener">GHSA-rqm5-mx2c-5fhp</a></td><td>0.0</td><td></td></tr>
    </tbody>
  </table>
</section><section>
  <h2>Hugging Face Trending Datasets</h2>
  <p class="metadata">Fetched at 2025-10-16T23:53:44Z • Source: <a href="https://huggingface.co/api/datasets" target="_blank" rel="noopener">https://huggingface.co/api/datasets</a> • Hash: <code>4b4cc33871c4b3cd497c500155aa79f37885aa24df2d895e1d18c0c80ab6ae9c</code></p>
  <p class="warning">401 Client Error: Unauthorized for url: https://huggingface.co/api/datasets?sort=downloads&amp;direction=-1&amp;limit=15 (Request ID: Root=1-68ff044f-2ac9bd2b0c97128b756dafe5;350061f2-d13a-494b-bef5-e9acb3243db0) Invalid credentials in Authorization header</p>
  <table>
    <thead>
      <tr><th>Dataset</th><th>Downloads</th><th>Likes</th></tr>
    </thead>
    <tbody>
      <tr><td><a href="https://huggingface.co/datasets/nebius/SWE-rebench" target="_blank" rel="noopener">nebius/SWE-rebench</a></td><td>3717252</td><td>35</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/SWE-Gym/SWE-Gym" target="_blank" rel="noopener">SWE-Gym/SWE-Gym</a></td><td>2041687</td><td>20</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/huggingface/documentation-images" target="_blank" rel="noopener">huggingface/documentation-images</a></td><td>2014781</td><td>86</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/chcorbi/helvipad" target="_blank" rel="noopener">chcorbi/helvipad</a></td><td>1478443</td><td>10</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/AquaV/genshin-voices-separated" target="_blank" rel="noopener">AquaV/genshin-voices-separated</a></td><td>1461805</td><td>8</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/banned-historical-archives/banned-historical-archives" target="_blank" rel="noopener">banned-historical-archives/banned-historical-archives</a></td><td>1411450</td><td>5</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim" target="_blank" rel="noopener">nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim</a></td><td>1339503</td><td>155</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified" target="_blank" rel="noopener">princeton-nlp/SWE-bench_Verified</a></td><td>1282494</td><td>218</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/lavita/medical-qa-shared-task-v1-toy" target="_blank" rel="noopener">lavita/medical-qa-shared-task-v1-toy</a></td><td>878541</td><td>21</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/Salesforce/wikitext" target="_blank" rel="noopener">Salesforce/wikitext</a></td><td>867174</td><td>505</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/IPEC-COMMUNITY/language_table_lerobot" target="_blank" rel="noopener">IPEC-COMMUNITY/language_table_lerobot</a></td><td>786287</td><td>0</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/IPEC-COMMUNITY/droid_lerobot" target="_blank" rel="noopener">IPEC-COMMUNITY/droid_lerobot</a></td><td>737114</td><td>6</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/hf-doc-build/doc-build" target="_blank" rel="noopener">hf-doc-build/doc-build</a></td><td>671848</td><td>12</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/applied-ai-018/pretraining_v1-omega_books" target="_blank" rel="noopener">applied-ai-018/pretraining_v1-omega_books</a></td><td>614296</td><td>2</td></tr>
      <tr><td><a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu" target="_blank" rel="noopener">HuggingFaceFW/fineweb-edu</a></td><td>597646</td><td>775</td></tr>
    </tbody>
  </table>
</section>
  </main>
  <footer><p>Datasets updated as part of the DeepTech Daily automation run.</p><p>Last refreshed at 2025-10-27T05:33:40Z.</p><ul><li><a href="../deeptech-daily/data/gpu_prices.json">gpu_prices.json</a></li><li><a href="../deeptech-daily/data/arxiv.yaml">arxiv.yaml</a></li><li><a href="../deeptech-daily/data/hf_trending.yaml">hf_trending.yaml</a></li><li><a href="../deeptech-daily/data/github_trending.yaml">github_trending.yaml</a></li><li><a href="../deeptech-daily/data/pwc_llm.yaml">pwc_llm.yaml</a></li><li><a href="../deeptech-daily/data/hn_ai.yaml">hn_ai.yaml</a></li><li><a href="../deeptech-daily/data/cves.yaml">cves.yaml</a></li><li><a href="../deeptech-daily/data/hf_datasets.yaml">hf_datasets.yaml</a></li></ul></footer>
</body>
</html>
